Objective:
Build a real-time interview assistant tool in Python that listens to audio from the microphone (or system audio), transcribes speech to text in real time, and sends detected questions to the OpenAI ChatGPT API for instant answer generation. The tool should provide a simple web interface for user control and answer display.
1. Project Requirements

    Real-Time Speech-to-Text:

        Use whisper (preferably faster-whisper for speed) or RealtimeSTT for low-latency transcription.

        If system audio capture is needed, use pyaudio or sounddevice to record from the default microphone or virtual audio device.

    Web Interface:

        Use Flask (or FastAPI) to create a simple web UI.

        UI should have a start/stop button for transcription.

        Display detected questions and AI-generated answers in real time.

    Question Detection:

        Detect questions by looking for question marks in the transcription or by detecting pauses.

        Alternatively, let the user manually trigger when a question is asked (e.g., by clicking a button).

    AI Answer Generation:

        Send the transcribed question to the OpenAI ChatGPT API.

        Display the generated answer(s) on the web UI.

        Allow the user to select or copy the best answer.

    Configuration:

        Provide a config file or environment variables for API keys and settings.

        Include clear instructions for setup and running the project.

2. Project Structure

text
interview-assistant/
├── app.py               # Main Flask/FastAPI server
├── requirements.txt     # Python dependencies
├── config.ini           # API keys and settings
├── static/              # Static files (CSS, JS)
├── templates/           # HTML templates
└── README.md            # Setup and usage instructions

3. Step-by-Step Implementation Plan

    Set up the Python environment:

        Create requirements.txt with flask, faster-whisper, openai, pyaudio/sounddevice.

    Build the web interface:

        Create app.py with routes for start/stop transcription and answer display.

        Add a simple HTML template with start/stop buttons and answer display area.

    Implement real-time speech-to-text:

        Use faster-whisper or RealtimeSTT to transcribe audio in real time.

        Start/stop transcription based on user input.

    Detect questions and send to ChatGPT:

        For simplicity, send every transcribed segment to ChatGPT when the user clicks a "send question" button.

        (Optional) Automatically detect questions by looking for question marks or pauses.

    Display answers:

        Show the AI-generated answer on the web UI.

        Allow the user to copy the answer.

    Add configuration:

        Use a config.ini file or environment variables for API keys.

    Write setup instructions:

        Add a README.md with steps to install dependencies, set API keys, and run the app.

4. Sample Code Outline

python
# app.py (Flask example)
from flask import Flask, render_template, request, jsonify
import openai
import faster_whisper
import sounddevice as sd
import numpy as np
import threading
import time

app = Flask(__name__)
openai.api_key = "YOUR_OPENAI_API_KEY"
model = faster_whisper.WhisperModel("small")

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/start_transcription', methods=['POST'])
def start_transcription():
    # Start audio recording and transcription in a background thread
    # For simplicity, this is a placeholder
    return jsonify({"status": "started"})

@app.route('/stop_transcription', methods=['POST'])
def stop_transcription():
    # Stop transcription
    return jsonify({"status": "stopped"})

@app.route('/send_question', methods=['POST'])
def send_question():
    question = request.json.get('question')
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": question}]
    )
    answer = response['choices'][0]['message']['content']
    return jsonify({"answer": answer})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)

xml
<!-- templates/index.html -->
<!DOCTYPE html>
<html>
<head>
    <title>Interview Assistant</title>
</head>
<body>
    <button id="start">Start Transcription</button>
    <button id="stop">Stop Transcription</button>
    <div id="question"></div>
    <button id="send">Send Question to AI</button>
    <div id="answer"></div>
    <script>
        // Add JavaScript to handle button clicks and API calls
    </script>
</body>
</html>

5. Next Steps

    Implement the audio recording and transcription logic.

    Connect the frontend to the backend for real-time updates.

    Test the tool with mock interviews.

    Optimize for low latency and accuracy.

This prompt is self-contained and provides a clear, actionable plan for building the project in Replit without needing constant direction.
